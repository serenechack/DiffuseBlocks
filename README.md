# EID-364_DiffuseBlocks
Physical model interface for Stable Diffusion and architectural model generation

Using PyTorch, OpenCV, Diffusers Library (https://huggingface.co/docs/diffusers/index)

![](https://github.com/TateLiang/EID-364_DiffuseBlocks/blob/main/Demo_Images/r1.png)
![](https://github.com/TateLiang/EID-364_DiffuseBlocks/blob/main/Demo_Images/r2.png)
![](https://github.com/TateLiang/EID-364_DiffuseBlocks/blob/main/Demo_Images/r3.png)


Play as Interface: Fostering Creativity and Imagination through Physical Engagement and Machine Learning

> Combining machine learning models to stimulate creativity through physical engagment
> Harness the power of generative machine learning to bring children's toys to life, and visualize the realsm of imagination they inspire
> Visualizing outputs in real time through the lens of generative models will transform abstract elements of play into more elaborate representations of an imagined world, like perceiving the world through the eyes of a chile
> Blocks become cities, playdoh becomes a serpent, tiles become a skyscraper

Objectives:
> A simple, intuitive interface between common creative children's toys and generative machine learning algorithms
> The interface relies on common toys and computer hardware such as blocks, play-doh and a computer with a web-cam
> one points a webcam at a structure of blocks for example, and type (or speak) "this is a castle" and watch as the generative algorithm returns one, or many, results. From there the user can refien the result
> Alternatively, they could select an image, but rebuild parts of it and ask for another image in the same style
